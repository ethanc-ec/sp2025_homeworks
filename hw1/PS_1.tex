\documentclass[10pt]{article}
\usepackage{amsmath, amssymb}

\begin{document}

\begin{center}
    \LARGE {Problem Set 1 – Supervised Learning} \\[1em]
    \Large{DS542 – DL4DS} \\[0.5em]
    \large Spring, 2025
\end{center}

\vspace{2em}

\noindent\textbf{Note:} Refer to the equations in the \textit{Understanding Deep Learning} textbook to solve the following problems.

\vspace{2em}

\section*{Problem 2.1}

To walk “downhill” on the loss function (equation 2.5), we measure its gradient with respect to the parameters $\phi_0$ and $\phi_1$. Calculate expressions for the slopes $\frac{\partial L}{\partial \phi_0}$ and $\frac{\partial L}{\partial \phi_1}$.

\text{L$[\phi]$ = $\sum_{i=1}^{I}$ ${(\phi_0 + \phi_1 x_i - y_i)}^2$}

\begin{align*}
\frac{\partial L}{\partial \phi_0} &= 2\sum_{i=1}^{I} (\phi_0 + \phi_1 x_i - y_i) \\[1em]
\frac{\partial L}{\partial \phi_1} &= 2\sum_{i=1}^{I} (\phi_0 + \phi_1 x_i - y_i)x_i
\end{align*}

\vspace{2em}

\section*{Problem 2.2}

Show that we can find the minimum of the loss function in closed-form by setting the expression for the derivatives from Problem 2.1 to zero and solving for $\phi_0$ and $\phi_1$.

To find the minimum of the loss function, we set the partial derivatives to zero and solve for $\phi_0$ and $\phi_1$.

Setting the derivatives to zero:

\begin{align*}
2\sum_{i=1}^{I} (\phi_0 + \phi_1 x_i - y_i) &= 0 \\
2\sum_{i=1}^{I} (\phi_0 + \phi_1 x_i - y_i)x_i &= 0
\end{align*}

Simplifying the first equation:
\begin{align*}
I\phi_0 + \phi_1\sum_{i=1}^{I} x_i &= \sum_{i=1}^{I} y_i \tag{1}
\end{align*}

Simplifying the second equation:
\begin{align*}
\phi_0\sum_{i=1}^{I} x_i + \phi_1\sum_{i=1}^{I} x_i^2 &= \sum_{i=1}^{I} x_iy_i \tag{2}
\end{align*}

Solving these simultaneous equations gives:
\begin{align*}
\phi_1 &= \frac{I\sum_{i=1}^{I} x_iy_i - \sum_{i=1}^{I} x_i\sum_{i=1}^{I} y_i}{I\sum_{i=1}^{I} x_i^2 - (\sum_{i=1}^{I} x_i)^2} \\[1em]
\phi_0 &= \frac{\sum_{i=1}^{I} y_i - \phi_1\sum_{i=1}^{I} x_i}{I}
\end{align*}


\end{document}
